{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating ChatBot Caddy \n",
    "\n",
    "## is it answering using faithfull information coming from the knowledge base?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yes, but first ask the LLM to answer given the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "golf_llm_caddy = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "def process_user_messages(json_file, system_message_file, context_folder):\n",
    "    \"\"\"\n",
    "    Processes user messages from a JSON file, retrieves context based on the Output label,\n",
    "    and constructs messages for a golf caddy chatbot using LangChain.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file containing user messages and labels.\n",
    "        system_message_content (str): System message content for the chatbot.\n",
    "        context_folder (str): Path to the folder containing context `.txt` files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing user messages, context, and processed chatbot input.\n",
    "    \"\"\"\n",
    "    # Load the dataset from the JSON file\n",
    "    with open(json_file, \"r\") as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    # Placeholder for processed results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each message in the dataset\n",
    "    for entry in dataset:\n",
    "        user_message = entry[\"User Message\"]\n",
    "        output_labels = entry[\"Output\"]\n",
    "\n",
    "        # Initialize context for this message\n",
    "        context = \"\\nCONTEXT ON HOW TO PLAY SHOT: \\n\"\n",
    "\n",
    "        # Retrieve context from corresponding text files\n",
    "        for label in output_labels:\n",
    "            file_name = label.lower().replace(\" \", \"_\") + \".txt\"\n",
    "            file_path = f\"{context_folder}/{file_name}\"\n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"r\") as context_file:\n",
    "                    context += context_file.read() + \"\\n\"  # Append the context from each file\n",
    "            except FileNotFoundError:\n",
    "                context += f\"[Context file {file_name} not found.]\\n\"\n",
    "\n",
    "        # Construct the HumanMessage with the retrieved context\n",
    "        human_message = HumanMessage(content=user_message+context)\n",
    "\n",
    "        print(human_message)\n",
    "\n",
    "        # Read the system message from the file\n",
    "        with open(system_message_file, \"r\") as file:\n",
    "            system_message_content = file.read()\n",
    "    \n",
    "        # Define the system message for the LLM\n",
    "        system_message = SystemMessage(content=system_message_content)\n",
    "\n",
    "        caddy_messages = [system_message, human_message]\n",
    "\n",
    "        golf_answer = golf_llm_caddy.invoke(caddy_messages)\n",
    "\n",
    "        # Log the input and response to a text file TODO automatically put right directory\n",
    "        with open(\"validation_datasets/golf_caddy/first_prompt/llm_classifier_log.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"User Message: {user_message}\\n\")\n",
    "            log_file.write(f\"LLM Output: {golf_answer.content}\\n\")\n",
    "            log_file.write(\"--------------------\\n\")\n",
    "\n",
    "\n",
    "        # Store the processed information\n",
    "        results.append({\n",
    "            \"User Message\": user_message,\n",
    "            \"Output\": output_labels,\n",
    "            \"Response\": golf_answer.content,\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_user_messages(\"validation_datasets/lie_classification/lie_classification.json\",\"system_messages/golf_caddy/first_prompt.txt\",\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to validation_datasets/golf_caddy/first_prompt/answers.json\n"
     ]
    }
   ],
   "source": [
    "# File path to save the JSON\n",
    "file_path = \"validation_datasets/golf_caddy/first_prompt/answers.json\"\n",
    "\n",
    "# Save the structure to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)\n",
    "\n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we perform the hallucination detection using LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "judge_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "def perform_hallucination_detection(results_file, system_message_file,judge_llm, context_folder):\n",
    "    \"\"\"\n",
    "    Iterates over existing results, retrieves context and LLM answers, and performs hallucination detection.\n",
    "\n",
    "    Args:\n",
    "        results_file (str): Path to the JSON file containing results with user messages, context, and LLM answers.\n",
    "        system_message_file (str): Path to the system message file for the hallucination detection judge.\n",
    "        judge_llm (object): LangChain LLM instance for hallucination detection.\n",
    "\n",
    "    Returns:\n",
    "        list: A new structure containing original results and hallucination evaluations.\n",
    "    \"\"\"\n",
    "    # Load the results dataset\n",
    "    with open(results_file, \"r\") as file:\n",
    "        results = json.load(file)\n",
    "\n",
    "    # Placeholder for new structure with hallucination detection results\n",
    "    new_structure = []\n",
    "\n",
    "    # Read the system message for the hallucination judge\n",
    "    with open(system_message_file, \"r\") as file:\n",
    "        system_message_content = file.read()\n",
    "\n",
    "    # Iterate over the existing results\n",
    "    for entry in results:\n",
    "        user_message = entry[\"User Message\"]\n",
    "        output_labels = entry[\"Output\"]\n",
    "        llm_answer = entry[\"Response\"]\n",
    "\n",
    "        context= ''\n",
    "        count=0\n",
    "        # Retrieve context from corresponding text files\n",
    "        for label in output_labels:\n",
    "            count+=1\n",
    "            file_name = label.lower().replace(\" \", \"_\") + \".txt\"\n",
    "            file_path = f\"{context_folder}/{file_name}\"\n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"r\") as context_file:\n",
    "                    context += context_file.read() + \"\\n\"  # Append the context from each file\n",
    "            except FileNotFoundError:\n",
    "                context += f\"[Context file {file_name} not found.]\\n\"\n",
    "\n",
    "        \n",
    "        # Construct the input for the hallucination judge\n",
    "        hallucination_message_content = (\n",
    "            f\"FACTS: {context.strip()}\\n\\n\"\n",
    "            f\"STUDENT ANSWER: {llm_answer.strip()}\\n\\n\"\n",
    "        )\n",
    "        hallucination_messages = [SystemMessage(content= system_message_content),HumanMessage(content=hallucination_message_content)]\n",
    "\n",
    "        print(f\"This is the row {count}: {hallucination_message_content}\")\n",
    "        \n",
    "        # Perform hallucination detection with the judge LLM\n",
    "        hallucination_judge_response = judge_llm.invoke(hallucination_messages)\n",
    "\n",
    "        # Log the hallucination detection process\n",
    "        with open(\"validation_datasets/hallucination_detection/answer_faithfullness/hallucination_log.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"User Message: {user_message}\\n\")\n",
    "            log_file.write(f\"Facts: {context.strip()}\\n\")\n",
    "            log_file.write(f\"LLM Answer: {llm_answer.strip()}\\n\")\n",
    "            log_file.write(f\"Hallucination Judge Response: {hallucination_judge_response.content}\\n\")\n",
    "            log_file.write(\"--------------------\\n\")\n",
    "\n",
    "        # Append the full information to the new structure\n",
    "        new_structure.append({\n",
    "            \"User Message\": user_message,\n",
    "            \"Output\": output_labels,\n",
    "            \"Facts\": context,\n",
    "            \"Response\": llm_answer,\n",
    "            \"Hallucination Detection\": hallucination_judge_response.content\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return new_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_results = perform_hallucination_detection(\"validation_datasets/golf_caddy/first_prompt/answers.json\",\"system_messages/hallucination_detection/answer_faithfullness.txt\",judge_llm, \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
