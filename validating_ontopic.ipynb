{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating LLM answer being on topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the two LLMs\n",
    "golf_classifier_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of situations possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predefined situations\n",
    "situations = [\n",
    "    \"GOLF_RELATED\",\n",
    "    \"GOLF_UNRELATED\",\n",
    "    \"SENSITIVE_TOPIC\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define LLM that detect which is the lie. From that we retrieve the correct context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_sensitive_topic(matched_situations):\n",
    "    if 'SENSITIVE_TOPIC' in matched_situations:\n",
    "        return ['SENSITIVE_TOPIC']\n",
    "    elif 'GOLF_UNRELATED' in matched_situations:\n",
    "        return ['GOLF_UNRELATED']\n",
    "    \n",
    "    return matched_situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_classifier(user_message, system_message_file):\n",
    "    \n",
    "    # Read the system message from the file\n",
    "    with open(system_message_file, \"r\") as file:\n",
    "        system_message_content = file.read()\n",
    "    \n",
    "    # Define the system message for the LLM\n",
    "    classifier_system_message = SystemMessage(content=system_message_content)\n",
    "\n",
    "    classifier_messages = [classifier_system_message, HumanMessage(content=user_message)]\n",
    "\n",
    "    # Get the classification from the LLM\n",
    "    classifier_response = golf_classifier_llm.invoke(classifier_messages)\n",
    "    \n",
    "\n",
    "    # Log the input and response to a text file TODO automatically put right directory\n",
    "    with open(\"validation_datasets/off_topic_classification/third_prompt/llm_classifier_log.txt\", \"a\") as log_file:\n",
    "        log_file.write(f\"User Message: {user_message}\\n\")\n",
    "        log_file.write(f\"LLM Output: {classifier_response.content}\\n\")\n",
    "        log_file.write(\"--------------------\\n\")\n",
    "\n",
    "    # Parse the classifier response for matching situations\n",
    "    matched_situations = [situation for situation in situations if situation in classifier_response.content]\n",
    "\n",
    "    matched_situations = prioritize_sensitive_topic(matched_situations)\n",
    "\n",
    "    return matched_situations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def process_validation_file(file_path, system_message_file):\n",
    "    # Load the validation data from the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        validation_data = json.load(file)\n",
    "\n",
    "    # Placeholder for storing results\n",
    "    results = []\n",
    "\n",
    "    # Process each entry in the JSON file\n",
    "    for entry in validation_data:\n",
    "        message = entry[\"Message\"]\n",
    "        label = entry[\"Label\"]\n",
    "\n",
    "        # Call the LLM classifier with the user message\n",
    "        classifier_result = call_llm_classifier(message,system_message_file)\n",
    "\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"Message\": message,\n",
    "            \"Label\": label,\n",
    "            \"Classifier Result\": classifier_result\n",
    "        })\n",
    "\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result = process_validation_file(\"validation_datasets/off_topic_classification/off_topic_classification.json\", \"system_messages/off_topic_classification/second_prompt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Message': 'What is the best club to use for a 150-yard shot?', 'Label': 'GOLF_RELATED', 'Classifier Result': ['GOLF_RELATED']}\n"
     ]
    }
   ],
   "source": [
    "print(validation_result[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_correctness(results):\n",
    "    \"\"\"\n",
    "    Calculate correctness for the classifier's output compared to the expected output,\n",
    "    explicitly penalizing incorrect classifications.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries, each containing \"Label\" and \"Classifier Result\".\n",
    "\n",
    "    Returns:\n",
    "        list: The input results with an additional \"Correctness\" key added to each dictionary.\n",
    "    \"\"\"\n",
    "    for result in results:\n",
    "        expected_label = result[\"Label\"]\n",
    "        classifier_result = result[\"Classifier Result\"]\n",
    "        \n",
    "        # Correctness calculation: 1 if the classifier result matches the expected label, else 0\n",
    "        correctness = 1.0 if expected_label in classifier_result else 0.0\n",
    "        \n",
    "        # Add the correctness metric to the result\n",
    "        result[\"Correctness\"] = correctness\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_global_correctness(results):\n",
    "    \"\"\"\n",
    "    Calculate the global correctness (average correctness across all results).\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries, each containing \"Correctness\" key.\n",
    "\n",
    "    Returns:\n",
    "        float: The average correctness score across all results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    \n",
    "    total_correctness = sum(result[\"Correctness\"] for result in results)\n",
    "    global_correctness = total_correctness / len(results)\n",
    "    return global_correctness\n",
    "\n",
    "def save_results_to_json(results, output_file):\n",
    "    \"\"\"\n",
    "    Save results with correctness to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries with \"Correctness\" key.\n",
    "        output_file (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result_with_correctness = calculate_correctness(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global correctness across the validation set is: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(f\"The global correctness across the validation set is: {calculate_global_correctness(validation_result_with_correctness)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO AUTOMATICALLY PUT RIGHT DIRECTORY\n",
    "\n",
    "save_results_to_json(validation_result_with_correctness,output_file='validation_datasets/off_topic_classification/second_prompt/validation_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
