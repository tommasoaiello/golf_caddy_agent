{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating LLM lie classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the two LLMs\n",
    "golf_classifier_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=256,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of situations possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predefined situations\n",
    "situations = [\n",
    "    \"BALL ABOVE FEET\",\n",
    "    \"BALL BELOW FEET\",\n",
    "    \"GREEN SIDE BUNKER SHOT\",\n",
    "    \"DOWNHILL LIE\",\n",
    "    \"UPHILL LIE\",\n",
    "    \"BALL IN ROUGH\",\n",
    "    \"FAIRWAY BUNKER\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define LLM that detect which is the lie. From that we retrieve the correct context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_classifier(user_message, system_message_file):\n",
    "    \n",
    "    # Read the system message from the file\n",
    "    with open(system_message_file, \"r\") as file:\n",
    "        system_message_content = file.read()\n",
    "    \n",
    "    # Define the system message for the LLM\n",
    "    classifier_system_message = SystemMessage(content=system_message_content)\n",
    "\n",
    "    classifier_messages = [classifier_system_message, HumanMessage(content=user_message)]\n",
    "\n",
    "    # Get the classification from the LLM\n",
    "    classifier_response = golf_classifier_llm.invoke(classifier_messages)\n",
    "    \n",
    "\n",
    "    # Log the input and response to a text file TODO automatically put right directory\n",
    "    with open(\"validation_datasets/lie_classification/second_prompt/llm_classifier_log.txt\", \"a\") as log_file:\n",
    "        log_file.write(f\"User Message: {user_message}\\n\")\n",
    "        log_file.write(f\"LLM Output: {classifier_response.content}\\n\")\n",
    "        log_file.write(\"--------------------\\n\")\n",
    "\n",
    "    # Parse the classifier response for matching situations\n",
    "    matched_situations = [situation for situation in situations if situation in classifier_response.content]\n",
    "\n",
    "    return matched_situations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GREEN SIDE BUNKER SHOT']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_message= 'The ball is in a bunker near the green.' \n",
    "call_llm_classifier(user_message, \"system_messages/lie_classification/second_prompt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def process_validation_file(file_path, system_message_file):\n",
    "    # Load the validation data from the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        validation_data = json.load(file)\n",
    "\n",
    "    # Placeholder for storing results\n",
    "    results = []\n",
    "\n",
    "    # Process each entry in the JSON file\n",
    "    for entry in validation_data:\n",
    "        user_message = entry[\"User Message\"]\n",
    "        expected_output = entry[\"Output\"]\n",
    "\n",
    "        # Call the LLM classifier with the user message\n",
    "        classifier_result = call_llm_classifier(user_message,system_message_file)\n",
    "\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"User Message\": user_message,\n",
    "            \"Expected Output\": expected_output,\n",
    "            \"Classifier Result\": classifier_result\n",
    "        })\n",
    "\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result = process_validation_file(\"validation_datasets/lie_classification/lie_classification.json\", \"system_messages/lie_classification/second_prompt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User Message': 'The ball is resting below my feet, and Iâ€™m worried it might slice when I hit it.', 'Expected Output': ['BALL BELOW FEET'], 'Classifier Result': ['BALL BELOW FEET']}\n"
     ]
    }
   ],
   "source": [
    "print(validation_result[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_correctness(results):\n",
    "    \"\"\"\n",
    "    Calculate correctness for the classifier's output compared to the expected output,\n",
    "    explicitly penalizing both extra and missing elements.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries, each containing \"Expected Output\" and \"Classifier Result\".\n",
    "\n",
    "    Returns:\n",
    "        list: The input results with an additional \"Correctness\" key added to each dictionary.\n",
    "    \"\"\"\n",
    "    for result in results:\n",
    "        expected = set(result[\"Expected Output\"])\n",
    "        classifier_result = set(result[\"Classifier Result\"])\n",
    "        \n",
    "        # Calculate correct matches, extra elements, and missing elements\n",
    "        correct_matches = expected.intersection(classifier_result)\n",
    "        extra_elements = classifier_result - expected\n",
    "        missing_elements = expected - classifier_result\n",
    "        \n",
    "        # Total elements considered (sum of correct matches, extras, and missing elements)\n",
    "        total_elements = len(correct_matches) + len(extra_elements) + len(missing_elements)\n",
    "        \n",
    "        # Correctness calculation\n",
    "        if total_elements > 0:\n",
    "            correctness = len(correct_matches) / total_elements\n",
    "        else:\n",
    "            correctness = 0.0  # Avoid division by zero\n",
    "        \n",
    "        # Add the correctness metric to the result\n",
    "        result[\"Correctness\"] = correctness\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_global_correctness(results):\n",
    "    \"\"\"\n",
    "    Calculate the global correctness (average correctness across all results).\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries, each containing \"Correctness\" key.\n",
    "\n",
    "    Returns:\n",
    "        float: The average correctness score across all results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    \n",
    "    total_correctness = sum(result[\"Correctness\"] for result in results)\n",
    "    global_correctness = total_correctness / len(results)\n",
    "    return global_correctness\n",
    "\n",
    "def save_results_to_json(results, output_file):\n",
    "    \"\"\"\n",
    "    Save results with correctness to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries with \"Correctness\" key.\n",
    "        output_file (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result_with_correctness = calculate_correctness(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global correctness across the validation set is: 0.9338235294117647\n"
     ]
    }
   ],
   "source": [
    "print(f\"The global correctness across the validation set is: {calculate_global_correctness(validation_result_with_correctness)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO AUTOMATICALLY PUT RIGHT DIRECTORY\n",
    "\n",
    "save_results_to_json(validation_result_with_correctness,output_file='validation_datasets/lie_classification/second_prompt/validation_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
